---
title: "Happy Moments"
author: "Jay Mu jm4610"
date: "September 9, 2018"
output: html_document
---

### Introduction
In this blog, I am willing to discuss what difference between different age groups' happy moments. 
I divide the dataset in to 4 age groups, for each group I did visulization and sentiments analysis.
Then, I did basic topic modeling for the 4 group by using LDA method.


### Step 0: Loading Package
Load all packages I need for this project!
```{r, warning=FALSE,message=FALSE}
library(ggplot2)
library(gridExtra)
library(tidytext)
library(wordcloud2)
library(tidyverse)
library(DT)
library(scales)
library(ngram)
library(tm)
library(topicmodels)
library(reshape2)
library(wordcloud)
```

### Step 1: Importing cleaned Data of survey and demo data
```{r, warning=FALSE,message=FALSE}
setwd("C:/Users/User/Documents/GitHub/Fall2018-Proj1-OKKING/output")
Happy <- read.csv("processed_moments.csv",header = TRUE)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```

### Step 2: Combing Survey Data and Demo Data
I combined survey data and demo data so that I created a new dataframe which contains both survey data and the clients's data
```{r combing and selecting data, echo=FALSE, warning=FALSE,message=FALSE}
happy<-Happy %>%
  inner_join(demo_data,by = "wid") %>%
  select(wid,
         original_hm,
         age,
         gender, 
         marital, 
         parenthood,
         country,
         reflection_period,
         ground_truth_category,
         predicted_category,
         text) %>%
  
  mutate(count = sapply(as.character(Happy$text),wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))

```

```{r changing mode,echo=FALSE, warning=FALSE,message=FALSE}
happy$original_hm <- as.character(happy$original_hm)
happy$age <- as.numeric(happy$age)
happy$predicted_category <- as.character(happy$predicted_category)
happy$text <- as.character(happy$text)
```

### Step 3: EDA, Age Histogram
I created a Histogram to see the age distribution of the sample, the majority clients have ages between 15 and 60. Therefore, I decide to split the sample into 4 different age groups: $under 25$;$between 25 and 30$; $between 30 and 40$ and $40 above$. In this way, the size of each group are not differ very much.
```{r,message=FALSE,echo=FALSE}
# Check the age distribution of the sample
ggplot(happy, aes(x=age)) + 
  geom_histogram(binwidth = 3, color="black", fill="green")+
  ggtitle("Ages Distribution ")+
  xlab("Age")+
  ylab("Frequency")+
  theme(panel.background = element_rect(fill = "lightblue",
                                colour = "lightblue",
                                size = 0.5, linetype = "solid"),
        panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
        panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white"))+
  xlim(min = 0, max = 100)
```
```{r,message=FALSE}
# Split 4 different groups according to ages
happy_under25<-happy[c(happy$age<25),]
happy_25_30<-happy[c(25<=happy$age&happy$age<30),]
happy_30_40<-happy[c(30<=happy$age&happy$age<40),]
happy_40above<-happy[c(happy$age>=40),]
```

### Step 4: Creating Bag of Words
I created bag of word for each age group by using my own fucnction $bagwordcount$.
```{r function to calculate the word count, message=FALSE}
# Write a function to calculate the frequencies of words in text
bagwordcount<-function(df){
  bag_word <- df %>%
    unnest_tokens(word,text)
  word_count <- bag_word %>%
    count(word, sort = T)
  return(word_count)
}

# Calculate words'frequencies in each age group
word_All<-bagwordcount(happy)
word_under25<-bagwordcount(happy_under25)
word_25_30<-bagwordcount(happy_25_30)
word_30_40<-bagwordcount(happy_30_40)
word_40above<-bagwordcount(happy_40above)
datatable(head(word_under25,6))
datatable(head(word_25_30,6))
datatable(head(word_30_40,6))
datatable(head(word_40above,6))
```
The above datatables show that for all for groups, "day", "friend" and "time" appear very frequently.
Therefore, there are no much difference among top 3 words for 4 age groups. Beside these 3 top words,
each age group has different speciality in words' frequencies.




### Step 5: WordCloud for 4 age groups (Excluded friend,day and time)
The wordclouds show that in each age group, which word appears most frequently, the larger the word is, the more frequently the word appears.
# Word Cloud of age group under 25
```{r, warning=FALSE,message=FALSE,echo = FALSE}

wordcloud2(word_under25[4:30,], size = 1, minSize = 0, gridSize =  0,
    fontFamily = 'Segoe UI', fontWeight = 'bold',
    color = 'random-dark', backgroundColor = "white",
    minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE,
    rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65,
    widgetsize = NULL, figPath = NULL, hoverFunction = NULL)
```


# Word Cloud of age group 25~30
```{r,echo = FALSE}
wordcloud2(word_25_30[4:30,], size = 1, minSize = 0, gridSize =  0,
    fontFamily = 'Segoe UI', fontWeight = 'bold',
    color = 'random-dark', backgroundColor = "white",
    minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE,
    rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65,
    widgetsize = NULL, figPath = NULL, hoverFunction = NULL)
```

# Word Cloud of age group 30~40
```{r,echo = FALSE}
wordcloud2(word_30_40[4:30,], size = 1, minSize = 0, gridSize =  0,
    fontFamily = 'Segoe UI', fontWeight = 'bold',
    color = 'random-dark', backgroundColor = "white",
    minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE,
    rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65,
    widgetsize = NULL, figPath = NULL, hoverFunction = NULL)
```

# Word Cloud of age group 40 above
```{r,echo = FALSE}
wordcloud2(word_40above[4:30,], size = 1, minSize = 0, gridSize =  0,
    fontFamily = 'Segoe UI', fontWeight = 'bold',
    color = 'random-dark', backgroundColor = "white",
    minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE,
    rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65,
    widgetsize = NULL, figPath = NULL, hoverFunction = NULL)
```



### Step 6: Words Sentiment Analysis
In this step, I created NRC word setiments histogram for each age group, I want to compare the sentiments distributions among 4 groups.
```{r Creating the NRC Words Frequency VS Words Sentiments for each group,warning=FALSE,message=FALSE,echo = FALSE}
par(mfrow=c(2, 2))
# Under 25 group
nrc_under25<-merge(x = word_under25,y = get_sentiments("nrc"),by.x = "word", by.y = "word")
names(nrc_under25)[names(nrc_under25) == "n"] <- "Frequency"
nrc_senti_under25 <- nrc_under25 %>%
                    select(Frequency, sentiment)
# colSums(table(nrc_senti_under25))
barplot(colSums(table(nrc_senti_under25)), col = "lightskyblue1",
        main = "Under 25",
        ylab = "Frequency", xlab = "Sentiment",
        border = NA, cex.names=0.7) 

# Between 25 and 30
nrc_25_30<-merge(x = word_25_30,y = get_sentiments("nrc"),by.x = "word", by.y = "word")
names(nrc_25_30)[names(nrc_25_30) == "n"] <- "Frequency"
nrc_senti_25_30 <- nrc_25_30 %>%
                    select(Frequency, sentiment)
# colSums(table(nrc_25_30))
barplot(colSums(table(nrc_senti_25_30)), col = "green",
        main = "25~30",
        ylab = "Frequency", xlab = "Sentiment",
        border = NA, cex.names=0.7)
# Between 30 and 40
nrc_30_40<-merge(x = word_30_40,y = get_sentiments("nrc"),by.x = "word", by.y = "word")
names(nrc_30_40)[names(nrc_30_40) == "n"] <- "Frequency"
nrc_senti_30_40 <- nrc_30_40 %>%
                    select(Frequency, sentiment)
# colSums(table(nrc_30_40))
barplot(colSums(table(nrc_senti_30_40)), col = "orange",
        main = "30~40",
        ylab = "Frequency", xlab = "Sentiment",
        border = NA, cex.names=0.7)


# Between 40 above
nrc_40above<-merge(x = word_40above,y = get_sentiments("nrc"),by.x = "word", by.y = "word")
names(nrc_40above)[names(nrc_40above) == "n"] <- "Frequency"
nrc_senti_40above <- nrc_40above %>%
                    select(Frequency, sentiment)
# colSums(table(nrc_45above))
barplot(colSums(table(nrc_senti_40above)), col = "purple",
        main = "40 above",
        ylab = "Frequency", xlab = "Sentiment",
        border = NA, cex.names=0.7)

```
From the histograms, we saw that the sentiments distributions for 4 groups are simillar. "Positive" and "Negative" words are the most common in all of the 4 groups. 



### Step 7: Topic Modeling
Beside the sentiments analysis, I did basic topic modeling for the 4 groups. First is to create a document term matrix.
```{r}
# Generate document-term matirx
doc<-rbind(paste(happy_under25$text,collapse = " "),paste(happy_25_30$text,collapse = " "),
           paste(happy_30_40$text,collapse = " "),paste(happy_40above$text,collapse = " "))

corpus<- Corpus(VectorSource(doc))
dtm <- DocumentTermMatrix(corpus)
rownames(dtm)<-c("under 25","25~30","30~40","40above")

```


After generating the document-term-matrix, I start to run LDA. In this step, I choose to have 7 topics since the original dataset $happy$ has 7 predicted_categories.
```{r}

# Set parameters for Gibbs sampling
burnin<-4000
iter<-2000
thin<-500
seed <-list(2003,5,63,100001,765)
nstart<-5
best<-TRUE
#Number of topics
k <- 7
#Run LDA using Gibbs sampling for group under 25
ldaOut <- LDA(dtm, k, method="Gibbs", control = list(burnin = burnin,iter = iter,
                                                                              thin = thin, seed = seed,
                                                                              nstart = nstart,best = best))

```

After running LDA, I checkd the per-document-per-word probability for each group.
```{r}
# per-document-per-word probability
group_gamma<-tidy(ldaOut, matrix = "gamma")
datatable(group_gamma)
```
For example, the first number under "gamma" means that a word generate from Topic 1 in "Under25" group has a probability of 0.11874

```{r}
# Visulize the per-document-per-word probability
group_gamma%>%
  ggplot(aes(factor(topic),gamma))+
  geom_boxplot()+
  theme(panel.background = element_rect(fill = "lightblue",
                                colour = "lightblue",
                                size = 0.5, linetype = "solid"),
        panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
        panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white"))+
  facet_wrap(~document)
```
The graph shows the topicprobability for each group. For example, in the group of "Under25", Topic 2 has the most probability which has a value about 0.5. This tells us that nearly half of the words in group Under 25 are from Topic 2. But we do not know what exactly the Topic 2 is, therfore I created a wordcloud for the 7 topics.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
### Wordcloud for the 7 topics
group_beta<-tidy(ldaOut, matrix = "beta")
ungroup_beta_terms <- group_beta %>%
  group_by(topic) %>%
  top_n(1000, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ungroup_beta_terms %>%
  mutate(topic = paste("Topic", topic)) %>%
  acast(term ~ topic, value.var = "beta", fill = 0) %>%
  comparison.cloud(colors = c("lightblue","red","green","yellow","blue","purple","orange"),
                   max.words = 1000)
```
From the wordcloud we see that the 7 topics are not overlap, so 7 topics is a good fit to the documents. The words close to Topic 2 are very likely to generate from Topic 2. For example, "love",
"free", "boyfriend","cat", "class" "finished" and "job" are very likely belongs to Topic 2. This result matches the previous argument. Since the Under 25 group people are most likely young students and labors, so the happy things for these group are more likely to relate to "study","job", "class" and "love".

### Finally, write out results 
I wrote some results for my further study in Excel format.
```{r}
#words to topics
ldaOut.topics <- as.matrix(topics(ldaOut,1))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"WordsToTopics.csv"))

#Top terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,30))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.matrix(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicsProbabilities.csv"))
```

































