# head(get_sentiments("afinn"))
repu.emotion.af <- merge(x = tdm.repu.overall, y = get_sentiments("afinn"), by.x = "term", by.y = "word")
names(repu.emotion.af)[names(repu.emotion.af) == "sum(count)"] <- "Frequency"
repu.sum.emotion.af <- repu.emotion.af %>%
select(Frequency, score)
# colSums(table(repu.sum.emotion.af))
barplot(colSums(table(repu.sum.emotion.af)), col = "lightskyblue3",
main = "Republican Word Frequency VS Word Sentiment Score",
ylab = "Frequency", xlab = "Sentiment Score",
border = NA, cex.names = 1)
ggplot() +
geom_point(data = inaug.demo, aes(x = Dates, y = Words), color = "blue") +
geom_point(data = inaug.repu, aes(x = Dates, y = Words), color = "red") +
labs(title = "Inaugural Dates vs Words Between two Parties",
x = "Inaugural Dates", y = "Inaugural Words")
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "tm", "topicmodels")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(rvest)
library(tibble)
# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
# In my laptop, I need to run the code below to library qdap and xlsx packages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_181')
library(qdap)
library(sentimentr)
library(gplots)
library(dplyr)
library(tm)
library(syuzhet)
library(factoextra)
library(beeswarm)
library(scales)
library(RColorBrewer)
library(RANN)
library(topicmodels)
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
print(R.version)
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
#### Nomination speeches
main.page=read_html("http://www.presidency.ucsb.edu/nomination.php")
# Get link URLs
nomin <- f.speechlinks(main.page)
#head(nomin)
#
#### Farewell speeches
main.page=read_html("http://www.presidency.ucsb.edu/farewell_addresses.php")
# Get link URLs
farewell <- f.speechlinks(main.page)
#head(farewell)
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
nomin.list=read.csv("../data/nominlist.csv", stringsAsFactors = FALSE)
farewell.list=read.csv("../data/farewelllist.csv", stringsAsFactors = FALSE)
speech.list=rbind(inaug.list, nomin.list, farewell.list)
speech.list$type=c(rep("inaug", nrow(inaug.list)),
rep("nomin", nrow(nomin.list)),
rep("farewell", nrow(farewell.list)))
speech.url=rbind(inaug, nomin, farewell)
speech.list=cbind(speech.list, speech.url)
View(speech.list)
View(speech.url)
nrow(speech.list)
nrow(speech.url)
speech.list=cbind(speech.list, speech.url[-126,])
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
text <- read_html(speech.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speech.list$fulltext[i]=text
# Create the file name
filename <- paste0("../data/fulltext/",
speech.list$type[i],
speech.list$File[i], "-",
speech.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
text <- read_html(speech.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speech.list$fulltext[i]<-text
# Create the file name
filename <- paste0("../data/fulltext/",
speech.list$type[i],
speech.list$File[i], "-",
speech.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "tm", "topicmodels")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(rvest)
library(tibble)
# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
# In my laptop, I need to run the code below to library qdap and xlsx packages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_181')
library(qdap)
library(sentimentr)
library(gplots)
library(dplyr)
library(tm)
library(syuzhet)
library(factoextra)
library(beeswarm)
library(scales)
library(RColorBrewer)
library(RANN)
library(tm)
library(topicmodels)
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches.
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
#### Nomination speeches
main.page=read_html("http://www.presidency.ucsb.edu/nomination.php")
# Get link URLs
nomin <- f.speechlinks(main.page)
#head(nomin)
#
#### Farewell speeches
main.page=read_html("http://www.presidency.ucsb.edu/farewell_addresses.php")
# Get link URLs
farewell <- f.speechlinks(main.page)
#head(farewell)
# Loop over each row in speech.list
speech.list$fulltext<-NULL
for(i in seq(nrow(speech.list))) {
text <- read_html(speech.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
speech.list$fulltext[i]<-text
# Create the file name
filename <- paste0("../data/fulltext/",
speech.list$type[i],
speech.list$File[i], "-",
speech.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
speech1=paste(readLines("../data/fulltext/SpeechDonaldTrump-NA.txt",
n=-1, skipNul=TRUE),
collapse=" ")
?wordcloud
# split the dataset by countries
split_country <- dlply(happy, .(country))
cbagwordcount<-function(df){
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
}
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
# split the dataset by countries
split_country <- dlply(happy, .(country))
library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(tidytext)
library(wordcloud2)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud2)
library(ngram)
library(shiny)
setwd("C:/Users/User/Documents/GitHub/Fall2018-Proj1-OKKING/output")
Happy <- read.csv("processed_moments.csv",header = TRUE)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
happy<-Happy %>%
inner_join(demo_data,by = "wid") %>%
select(wid,
original_hm,
age,
gender,
marital,
parenthood,
country,
reflection_period,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(as.character(Happy$text),wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
happy$original_hm <- as.character(happy$original_hm)
happy$age <- as.numeric(happy$age)
happy$predicted_category <- as.character(happy$predicted_category)
happy$text <- as.character(happy$text)
cbagwordcount<-function(df){
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
}
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = TRUE)
View(happy)
bagwordcount<-function(df){
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
}
bagwordcount<-function(df){
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
}
bag_word_pool <- happy %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = TRUE)
Word_count_pool <- bagwordcount(happy)
?count
#library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(tidytext)
library(wordcloud2)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud2)
library(ngram)
library(shiny)
setwd("C:/Users/User/Documents/GitHub/Fall2018-Proj1-OKKING/output")
Happy <- read.csv("processed_moments.csv",header = TRUE)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
happy<-Happy %>%
inner_join(demo_data,by = "wid") %>%
select(wid,
original_hm,
age,
gender,
marital,
parenthood,
country,
reflection_period,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(as.character(Happy$text),wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
happy$original_hm <- as.character(happy$original_hm)
happy$age <- as.numeric(happy$age)
happy$predicted_category <- as.character(happy$predicted_category)
happy$text <- as.character(happy$text)
bagwordcount<-function(df){
bag_word_pool <- df %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
}
Word_count_pool <- bagwordcount(happy)
#library(plyr)
#library(dplyr)
library(ggplot2)
library(gridExtra)
library(tidytext)
library(wordcloud2)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud2)
library(ngram)
library(shiny)
setwd("C:/Users/User/Documents/GitHub/Fall2018-Proj1-OKKING/output")
Happy <- read.csv("processed_moments.csv",header = TRUE)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
happy<-Happy %>%
inner_join(demo_data,by = "wid") %>%
select(wid,
original_hm,
age,
gender,
marital,
parenthood,
country,
reflection_period,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(as.character(Happy$text),wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
happy$original_hm <- as.character(happy$original_hm)
happy$age <- as.numeric(happy$age)
happy$predicted_category <- as.character(happy$predicted_category)
happy$text <- as.character(happy$text)
bagwordcount<-function(df){
bag_word_pool <- df %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word, sort = T)
}
Word_count_pool <- bagwordcount(happy)
bagwordcount<-function(df){
bag_word_pool <- df %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word)
}
Word_count_pool <- bagwordcount(happy)
bagwordcount<-function(df){
bag_word_pool <- df %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word,sort=T)
}
Word_count_pool <- bagwordcount(happy)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(tidytext)
library(wordcloud2)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud2)
library(ngram)
library(shiny)
setwd("C:/Users/User/Documents/GitHub/Fall2018-Proj1-OKKING/output")
Happy <- read.csv("processed_moments.csv",header = TRUE)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
happy<-Happy %>%
inner_join(demo_data,by = "wid") %>%
select(wid,
original_hm,
age,
gender,
marital,
parenthood,
country,
reflection_period,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(as.character(Happy$text),wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
happy$original_hm <- as.character(happy$original_hm)
happy$age <- as.numeric(happy$age)
happy$predicted_category <- as.character(happy$predicted_category)
happy$text <- as.character(happy$text)
datatable(happy)
bag_of_words_text <- happy %>%
unnest_tokens(word, text)
word_count_text <- bag_of_words_text %>%
count(word, sort = TRUE)
library(ggraph)
library(igraph)
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
happy<-Happy %>%
inner_join(demo_data,by = "wid") %>%
select(wid,
original_hm,
age,
gender,
marital,
parenthood,
country,
reflection_period,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(as.character(Happy$text),wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
happy$original_hm <- as.character(happy$original_hm)
happy$age <- as.numeric(happy$age)
happy$predicted_category <- as.character(happy$predicted_category)
happy$text <- as.character(happy$text)
bagwordcount<-function(df){
bag_word_pool <- df %>%
unnest_tokens(word, text)
word_count_pool <- bag_word_pool %>%
count(word,sort=T)
}
bagwordcount<-function(df){
bag_word <- df %>%
unnest_tokens(word, text)
word_count <- bag_word %>%
count(word,sort=T)
return(word_count)
}
Word_count_pool <- bagwordcount(happy)
bagwordcount<-function(df){
bag_word <- df %>%
unnest_tokens(word, text)
word_count <- bag_word %>%
count(word,sort=T)
return(word_count)
}
sapply(happy, mode)
