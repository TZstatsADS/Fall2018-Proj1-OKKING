mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict)
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
#putting it together in a format easy to use
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
datatable(hm_data)
#This section has been adapted and modified from the professor's HappyDB_RShiny.rmd file
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
# Combine both the data sets and keep the required columns for analysis
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(hmid,
wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
datatable(hm_data)
### Create a bag of words using the text data
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
### Create bigrams using the text data
hm_bigrams <- hm_data %>%
filter(count != 1) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigram_sep <- hm_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigram_sep2 <-bigram_sep %>%
filter(word1=='i')
#lemmatizing word2 if 'i's:
bigram_sep2$lemma<-lemmatize_words(bigram_sep2$word2)
# Checking how many start with 'i'and what verb follows
bigram_counts <- bigram_sep2 %>%
count(word1, lemma, sort = TRUE)
#wordcloud of top ten action words following 'i'. No giving words at all
bigram_counts[,c(2,3)] %>%
slice(1:30) %>%
wordcloud2(size = 0.6,
rotateRatio = 0)
#VAD Scores for HappyDB
#adding vad scores for each hmid
vadurlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/vad.csv'
vadfile <- read_csv(vadurlfile)
happy_vad<-vadfile[!is.na(vadfile$valency),]
happy_vad<-happy_vad[!is.na(happy_vad$dominance),]
happy_vad<-happy_vad[!is.na(happy_vad$arousal),]
i_vad<-sqldf('select happy_vad.hmid,bigram_sep2.word1,bigram_sep2.lemma, happy_vad.valency,happy_vad.dominance,happy_vad.arousal from happy_vad join bigram_sep2 on happy_vad.hmid=bigram_sep2.hmid')
#Investigating the difference between happiness because of giving and taking
give_take<-i_vad[which(i_vad$lemma=='help'|i_vad$lemma=='give'|i_vad$lemma=='feel'|i_vad$lemma=='share'|i_vad$lemma=='get'),]
#tukey test below shows that have-get and give-feel are not statistically different, but others are.
a1<-aov(give_take$valency~give_take$lemma)
tuk<-TukeyHSD(x=a1,'give_take$lemma')
#Plotting the findings in a violin plot
# suppressPackageStartupMessages({library(plotly)})
# library(plotly)
find1 <- give_take %>%
plot_ly(x = ~lemma,y = ~valency,split = ~lemma,type = 'violin',box = list(visible = T),meanline = list(visible = T ))%>%
layout(xaxis = list(title = "Give/Take Actions"),yaxis = list(title = "Valency",zeroline = F),showlegend=FALSE)
find1
getwd()
#Testing how 24hrs and 3month arousal values differ:
#hm_df <- read_csv("C:/Users/Deepika/Documents/ADS/Project 1/Fall2018-Proj1-deepikanambo3/doc/processed_moments.csv")
hm_df<-read.csv('processed_moments.csv',header = TRUE)
ref_df<-sqldf('select happy_vad.*,hm_df.reflection_period from hm_df join happy_vad where hm_df.hmid=happy_vad.hmid')
ref_ind<-sample(1:nrow(ref_df),6000)
ref_df<-ref_df[ref_ind,]
ref_df$refid<-seq(1:nrow(ref_df))
#Testing how males and females arousal values differ:
demo_df<-demo_data
demo_df<-sqldf('select demo_df.*,hm_df.hmid from hm_df join demo_df where hm_df.wid=demo_df.wid')
demo<-sqldf('select happy_vad.*,demo_df.* from demo_df join happy_vad where demo_df.hmid=happy_vad.hmid')
demo<-demo[,-1]
#Taking a sample of 6000
demo_ind<-sample(1:nrow(demo),6000)
demodf<-demo[demo_ind,]
demodf$demoid<-seq(1:nrow(demodf))
#Observing how valency and dominance vary by gender and country
cont_df<-demodf[which(demodf$country=='IND'|demodf$country=='CAN'|demodf$country=='VEN'|demodf$country=='USA'),]
ggplot(cont_df, aes(x=country, y=valency,fill=gender))+geom_boxplot()
ggplot(cont_df, aes(x=country, y=dominance,fill=gender))+geom_boxplot()
#Observing how valency varies by mrital status and country
ggplot(cont_df, aes(x=country, y=valency,fill=marital))+geom_boxplot()
#testing genderwise reflection periods
gen_ref_df<-sqldf('select ref_df.*,demo_df.gender from ref_df join demo_df where ref_df.hmid==demo_df.hmid  ')
#Testing if arousal values are different for different reflection periods
gen_ref_df<-gen_ref_df[which(gen_ref_df$gender=='f'|gen_ref_df$gender=='m'),]
#statistically different arousals
aov_ref<-aov(gen_ref_df$arousal~gen_ref_df$reflection_period)
tuk_ref<-TukeyHSD(x=aov_ref,'gen_ref_df$reflection_period')
tuk_ref
find2 <- ggplot(gen_ref_df, aes(x = reflection_period, y = arousal,fill=gender)) +
geom_boxplot(alpha=0.5,width=1/length(unique(gen_ref_df$reflection_period)))+
scale_y_continuous(name = "Arousal",
breaks = seq(1.5, 8, 2),
limits=c(1.5, 8)) +
scale_x_discrete(name = "Reflection Period") +
theme_bw() +
theme(panel.grid.major = element_line(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
plot.title = element_text(size = 14, family = "Tahoma", face = "bold"),
text=element_text(family = "Tahoma"),
axis.title = element_text(face="bold"),
axis.text.x = element_text(colour="black", size = 11),
axis.text.y = element_text(colour="black", size = 9),
axis.line = element_line(size=0.5, colour = "black"))+
scale_fill_brewer(palette = "Accent")
find2
#Calculating VAD scores for tweets
#http://help.sentiment140.com/for-students - this has the data set 1.6 million tweets and classified as 0 = negative, 2 = neutral, 4 = positive. Loading it into R and selecting only the positive ones
#all_twets<-read.csv('C:/Users/Deepika/Documents/ADS/Project 1/Fall2018-Proj1-deepikanambo3/twitterData/twitter_data.csv',header=FALSE)
all_twets<-read.csv('~trainingandtestdata/twitter_data.csv',header=FALSE)
View(give_take)
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
library(knitr)
library(cowplot)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
hm_data = read_csv("C:/Users/User/Desktop/WD/Happy-Moments-ib2392/Project1-RNotebook/output/processed_moments.csv")
urlfile='https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data = read_csv(urlfile)
hm_data = hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
#a lot of the original moments were duplicated.  In the following line, I try to remove as many as I can.
hm_data=hm_data[!duplicated(hm_data$original_hm),]
#some ages say 2 or 2.0, or 3 or 3.0, which I think were supposed to be 20 or 30 so I change those:
hm_data$age[which(as.numeric(hm_data$age)==2)]=20
hm_data$age[which(as.numeric(hm_data$age)==3)]=30
paste("\\b","input","\\b",sep="")
paste("\\b",input,"\\b",sep="")
#import the list of all the boy and girl names:
names=read.csv("C:/Users/User/Desktop/WD/Happy-Moments-ib2392/Project1-RNotebook/output
/names.csv",header=FALSE)
#import the list of all the boy and girl names:
names=read.csv("C:/Users/User/Desktop/WD/Happy-Moments-ib2392/Project1-RNotebook/output/names.csv",header=FALSE)
#change names from factor to string:
names=sapply(names,as.character)
#using analysis of word and bigram counts and just everyday knowledge, here is a compiled list of words that could indicate a social event:
#The ones that use regular expressions instead of a straightforward quote are supposed to generalize: in law/inlaw/in-law, and ___-year-old/___ year old, where ____ can be written as a number or a word and be separated by a space or a hyphen.
social_words=c("family","friend","together","people","girlfriend","boyfriend","brother","sister","daughter","\\bson\\b","mother","father","\\bmom\\b","\\bmommy\\b","\bbdad\bb","\\bdaddy\\b","husband","wife","aunt","uncle","girl","guy","niece","nephew","fiance","parent","coworker","cousin","grandparent","grandmother","grandfather","grandma","grandpa","relative","person","partner","neighbor","neighbour","classmates","sibling","spouse","chat","party","acquaintance","date","roommate","grandson","granddaughter","soulmate","twin","in[[:space:]]*[[:punct:]]*law","child", "kid","kids","my [0-9A-Za-z]+[[:punct:]]*[[:space:]]*year[[:punct:]]*[[:space:]]*old","\\bmate\\b","colleague","buddy","buddies","\\bcrush\\b","freind","\\blover\\b")
#now search through the dataset and save all moments that have a name from the list of names that we have, ignoring capital or lowercase.  A list of indices will be saved in names_matches_ignore_case.
func=function(input)
{
string=paste("\\b",input,"\\b",sep="")
return(grep(pattern=string,x=hm_data$original_hm,ignore.case=TRUE))
}
#names_matches_ignore_case=sapply(names, func)
#names_matches_ignore_case=as.numeric(unlist(names_matches_ignore_case))
#write.csv(names_matches_ignore_case,"Github/Project1-RNotebook/output/name_matches.csv",row.names=FALSE)
#To make running it go faster, I ran lines 79 to 85 once and then just saved the result using line 86, and then each time I ran the code going forward, I would just run line 89.  But if you would like to run it from scratch, uncomment lines 79 to 85, and run those instead of line 89.
names_matches_ignore_case=unique(as.numeric(unlist(read.csv("Github/Project1-RNotebook/output/name_matches.csv"))))
#import the list of all the boy and girl names:
names=read.csv("C:/Users/User/Desktop/WD/Happy-Moments-ib2392/Project1-RNotebook/output/names.csv",header=FALSE)
#change names from factor to string:
names=sapply(names,as.character)
#using analysis of word and bigram counts and just everyday knowledge, here is a compiled list of words that could indicate a social event:
#The ones that use regular expressions instead of a straightforward quote are supposed to generalize: in law/inlaw/in-law, and ___-year-old/___ year old, where ____ can be written as a number or a word and be separated by a space or a hyphen.
social_words=c("family","friend","together","people","girlfriend","boyfriend","brother","sister","daughter","\\bson\\b","mother","father","\\bmom\\b","\\bmommy\\b","\bbdad\bb","\\bdaddy\\b","husband","wife","aunt","uncle","girl","guy","niece","nephew","fiance","parent","coworker","cousin","grandparent","grandmother","grandfather","grandma","grandpa","relative","person","partner","neighbor","neighbour","classmates","sibling","spouse","chat","party","acquaintance","date","roommate","grandson","granddaughter","soulmate","twin","in[[:space:]]*[[:punct:]]*law","child", "kid","kids","my [0-9A-Za-z]+[[:punct:]]*[[:space:]]*year[[:punct:]]*[[:space:]]*old","\\bmate\\b","colleague","buddy","buddies","\\bcrush\\b","freind","\\blover\\b")
#now search through the dataset and save all moments that have a name from the list of names that we have, ignoring capital or lowercase.  A list of indices will be saved in names_matches_ignore_case.
func=function(input)
{
string=paste("\\b",input,"\\b",sep="")
return(grep(pattern=string,x=hm_data$original_hm,ignore.case=TRUE))
}
#names_matches_ignore_case=sapply(names, func)
#names_matches_ignore_case=as.numeric(unlist(names_matches_ignore_case))
#write.csv(names_matches_ignore_case,"Github/Project1-RNotebook/output/name_matches.csv",row.names=FALSE)
#To make running it go faster, I ran lines 79 to 85 once and then just saved the result using line 86, and then each time I ran the code going forward, I would just run line 89.  But if you would like to run it from scratch, uncomment lines 79 to 85, and run those instead of line 89.
names_matches_ignore_case=unique(as.numeric(unlist(read.csv("C:/Users/User/Desktop/WD/Happy-Moments-ib2392/Project1-RNotebook/output/name_matches.csv"))))
#"unique" is used because if a moment has more than one name in it, then it is counted more than once, but we just want the list of unique indices that contain names.
#now search through the dataset and compile a list of all happy moments that include one of the social words from the social_words vector. Save the indices:
social_words_matches=c()
for (i in 1:length(social_words))
{
social_words_matches=c(social_words_matches,grep(social_words[i],hm_data$original_hm,ignore.case=TRUE))
}
#Now search through for all moments that have "we" in it, and save the indices:
we=grep("\\bwe\\b",hm_data$original_hm,ignore.case=TRUE) #all statements that include "we"
#Search through for all moments that have "and I" in it, and save the indices:
and_i=grep("\\band\\b \\bi\\b[^']+.]*",hm_data$original_hm,ignore.case=TRUE) #all statements that include "and I"
#Search through for all moments that have "with" in it, and save the indices:
with=grep("\\bwith\\b",hm_data$original_hm,ignore.case=TRUE) #all statements that include "with"
#Search through for all moments that have "meet/met/meeting" in it, and save the indices:
meet=union(union(grep("\\bmeet\\b",hm_data$original_hm,ignore.case=TRUE),grep("\\bmet\\b",hm_data$original_hm,ignore.case=TRUE)),grep("\\bmeeting\\b",hm_data$original_hm,ignore.case=TRUE)) #all statements that include "meet" or "met" or "meeting"
#Here are some scenarios that I ran to do my analysis.  But only the ones that were saved into a variable name were used in the final output:
#statements that include "we" but no reference to social words.  This gave me an idea of some moments that might say we but don't mean they're social, for example, something like "I went to go visit the town we grew up in"
setdiff(hm_data$original_hm[we],hm_data$original_hm[social_words_matches])
#statements that include "and I" but no reference to social words.  Gave me an idea of many moments that say "and I" but are probably not social, like "I got an A and I am very happy".  The social ones would probably be like "my brother and I", or "John and I"
setdiff(hm_data$original_hm[and_i],hm_data$original_hm[social_words_matches])
#statements that use ((at least one of "meet","with", "and I", or "we"), AND a person's name), OR (a social word).  Ended up being the pattern I used to classify a social moment:
social_moments=union(intersect(union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet])),hm_data$original_hm[names_matches_ignore_case]),hm_data$original_hm[social_words_matches])
#same as above except this stores the indices instead of the moments themselves:
social_moments_indices=union(intersect(union(with,union(union(and_i,we),meet)),names_matches_ignore_case),social_words_matches)
#stores all moments that were not classified as a social moment above:
not_social_moments=setdiff(hm_data$original_hm,union(intersect(union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet])),hm_data$original_hm[names_matches_ignore_case]),hm_data$original_hm[social_words_matches]))
#same as above except stores the indices instead of the moments themselves:
not_social_moments_indices=setdiff(1:nrow(hm_data),union(intersect(union(with,union(union(and_i,we),meet)),names_matches_ignore_case),social_words_matches))
#statements that say "we","with","and I", or "meet" but are not classified as social moments.  Allows me to do a spot check to make sure I'm not missing too many of these that should be social moments but were incorrectly classified.
setdiff(intersect(hm_data$original_hm,union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet]))),union(intersect(union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet])),hm_data$original_hm[names_matches_ignore_case]),hm_data$original_hm[social_words_matches]))
#statements that include "we" but are not classified as social moments.  Allows me to do a spot check specifically for "we" and see if there were a lot misclassified.
setdiff(intersect(hm_data$original_hm,hm_data$original_hm[we]),union(intersect(union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet])),hm_data$original_hm[names_matches_ignore_case]),hm_data$original_hm[social_words_matches]))
#reference to a name without it being classified as a social moment:
#basically can be represented as: setdiff(has a name in it, classified as social moment)
setdiff(hm_data$original_hm[names_matches_ignore_case],union(intersect(union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet])),hm_data$original_hm[names_matches_ignore_case]),hm_data$original_hm[social_words_matches]))
#reference to a name and is classified as social moment:
#can be thought of as: intersect(has a name in it, classified as a social moment)
intersect(hm_data$original_hm[names_matches_ignore_case],union(intersect(union(hm_data$original_hm[with],union(union(hm_data$original_hm[and_i],hm_data$original_hm[we]),hm_data$original_hm[meet])),hm_data$original_hm[names_matches_ignore_case]),hm_data$original_hm[social_words_matches]))
#Using the social moment indices determined above, a new variable in the dataset called "type" is created, which classifies the moment as social or individual:
type=rep(NA,nrow(hm_data)) #initializes the vector
type[social_moments_indices]="social"
type[not_social_moments_indices]="individual"
hm_data=data.frame(hm_data[1:11],type) #add to data frame.  [1:11] is used so that if type was already added but needs to be re added because it was updated, it wouldn't save the old type, just the 11 original columns and the new type
df=data.frame(c("Social","Individual"),c(mean(hm_data$type=="social"),mean(hm_data$type=="individual"))) #stores a 2x2 data frame that basically just says social and individual with the proportions of each
colnames(df)=c("Type","Percentage")
#use this new dataframe as a plot:
ggplot(df,aes(x=Type,y=Percentage,fill="red")) + geom_bar(stat="identity",show.legend=FALSE) + geom_text(aes(x=Type,y=Percentage,label=paste(round(100*Percentage,1),"%"))) + ggtitle ("Percentage of Moments Classified as Social or Individual")
#This is a function where you input a wordtype ("word" or "bigram"), and a socialtype ("social" or "individual"), and it will give you the word count or bigram count for that specified subgroup
word_bigrams=function(wordtype,socialtype)
{
stopifnot(wordtype %in% c("word","bigram"))
stopifnot(socialtype %in% c("social","individual"))
hm_subset=hm_data[hm_data$type==socialtype,]
if (wordtype=="word")
{
bag_of_words = hm_subset %>%
unnest_tokens(word, text)
word_count = bag_of_words %>%
count(word, sort = TRUE)
return(word_count)
}
else
{
bigrams = hm_subset %>%
filter(count != 1) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigram_counts = bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
count(word1, word2, sort = TRUE)
return (bigram_counts)
}
}
set.seed(1234)
wordcloud(words = word_bigrams("word","social")$word, freq = word_bigrams("word","social")$n, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = word_bigrams("word","individual")$word, freq = word_bigrams("word","individual")$n, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
bigram_social=word_bigrams("bigram","social")[1:20,]
bigram_social=data.frame(paste(bigram_social$word1,bigram_social$word2),bigram_social$n)
colnames(bigram_social)=c("Bigram","Frequency")
bigram_individual=word_bigrams("bigram","individual")[1:20,]
bigram_individual=data.frame(paste(bigram_individual$word1,bigram_individual$word2),bigram_individual$n)
colnames(bigram_individual)=c("Bigram","Frequency")
social_plot=ggplot(bigram_social, aes(x=reorder(Bigram,Frequency),y=Frequency,fill="red"))+geom_bar(stat="identity", show.legend = FALSE) +coord_flip() + labs(x=NULL) + ggtitle("Social Moment Bigrams")
indiv_plot=ggplot(bigram_individual, aes(x=reorder(Bigram,Frequency),y=Frequency,fill="red"))+geom_bar(stat="identity", show.legend = FALSE) +coord_flip() + labs(x=NULL) + ggtitle("Indiv. Moment Bigrams")
plot_grid(social_plot,indiv_plot)
#Create a vector that shows the proportions of social/individual moments grouped by gender:
gender_type <- hm_data %>%
group_by(gender,type) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
#stacked percentage barplot to show relationship between gender and moment type:
gender_plot=ggplot(gender_type,aes(x=gender,y=perc,fill=type)) + geom_bar(stat="identity") + labs(x="gender",y="percentage") + guides(fill=guide_legend(title="Type"))+ggtitle("Gender") + geom_text(aes(gender,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
parenthood_type <- hm_data %>%
group_by(parenthood,type) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
parenthood_plot=ggplot(parenthood_type,aes(x=parenthood,y=perc,fill=type)) + geom_bar(stat="identity")+labs(x="parenthood",y="percentage") + guides(fill=guide_legend(title="Type"))+ggtitle("Parenthood") + geom_text(aes(parenthood,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
marital_type <- hm_data %>%
group_by(marital,type) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
marital_plot=ggplot(marital_type,aes(x=marital,y=perc,fill=type)) + geom_bar(stat="identity") + labs(x="marital status",y="percentage") + guides(fill=guide_legend(title="Type"))+ggtitle("Marital Status") + geom_text(aes(marital,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
usa_type <- hm_data[!is.na(hm_data$country),] %>%
group_by(country=="USA",type) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
colnames(usa_type)=c("Country","type","count","perc")
usa_type$Country[usa_type$Country==TRUE]="USA"
usa_type$Country[usa_type$Country==FALSE]="Not USA"
usa_plot=ggplot(usa_type,aes(x=Country,y=perc,fill=type)) + geom_bar(stat="identity") +labs(x="Country",y="percentage") + guides(fill=guide_legend(title="Type"))+ggtitle("Country")+geom_text(aes(Country,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
#create discrete age group levels:
age_less_25=which(hm_data$age<=25)
age_25_35=which(hm_data$age>25 & hm_data$age<=35)
age_35_45=which(hm_data$age>35 & hm_data$age<=45)
age_45_60=which(hm_data$age>45 & hm_data$age<=60)
age_60_older=which(hm_data$age>60)
age_group=rep(NA,nrow(hm_data))
age_group[age_less_25]="< 25"
age_group[age_25_35]="25-35"
age_group[age_35_45]="35-45"
age_group[age_45_60]="45-60"
age_group[age_60_older]="60+"
hm_data=data.frame(hm_data[1:12],age_group)
age_type <- hm_data[!is.na(hm_data$age_group),] %>%
group_by(age_group,type) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
age_plot=ggplot(age_type,aes(x=age_group,y=perc,fill=type)) + geom_bar(stat="identity") + labs(x="Age Group",y="percentage") + guides(fill=guide_legend(title="Type"))+ggtitle("Age Groups") + geom_text(aes(age_group,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
plot_grid(gender_plot,usa_plot)
age_plot
plot_grid(parenthood_plot,marital_plot)
library(dplyr)
marital_parenthood <- hm_data %>%
group_by(marital,parenthood) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
ggplot(marital_parenthood,aes(marital,perc,fill=parenthood)) + geom_bar(stat="identity")+labs(x="marital status",y="percentage") + guides(fill=guide_legend(title="Parenthood"))+ggtitle("Percentage of Single and Married People That Are Parents") + geom_text(aes(marital,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
#NAME ANALYSIS CHUNK
#categorize whether a moment has a name in it or not:
name=rep(NA,nrow(hm_data))
name[names_matches_ignore_case]="Has Name"
name[-names_matches_ignore_case]="No Name"
hm_data=data.frame(hm_data[,1:13],name)
name_perc <- hm_data %>%
group_by(name) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
num_names=ggplot(name_perc,aes(x=name, y = perc, fill = "red"))+geom_bar(stat="identity",show.legend=FALSE) + labs(y="percentage") + ggtitle ("% Moments W/ Names") + geom_text(aes(name,perc,label=paste(round(100*perc,1),"%")))
name_type <- hm_data %>%
group_by(name,type) %>%
summarise(count=n()) %>%
mutate(perc=count/sum(count))
name_plot=ggplot(name_type,aes(x=name,y=perc,fill=type)) + geom_bar(stat="identity") + labs(x=NULL,y="percentage") + guides(fill=guide_legend(title="Type"))+ggtitle("Moment Type With/Without Names") + geom_text(aes(name,perc,label=paste(round(100*perc,1),"%")),position=position_stack(vjust=0.5))
plot_grid(num_names,name_plot)
library(datasets)
ui1<-shinyUI(
pageWithSidebar(
headerPanel("Hello Shiny"),
sidebarPanel(
selectInput("dataset","choose a dataset:", choices = c("rock","pressure","cars")),
numericInput("obs","Number of observations to View:",10)
),
mainPanel(
verbatimTextOutput("summary"),
tableOutput("view")
)
)
)
server1<-shinyServer(function(input,output){
datasetInput<-reactive({
switch (input$dataset,
"rock" = rock,
"pressure" = pressure,
"cars" = cars)
}),
library(datasets)
ui1<-shinyUI(
pageWithSidebar(
headerPanel("Hello Shiny"),
sidebarPanel(
selectInput("dataset","choose a dataset:", choices = c("rock","pressure","cars")),
numericInput("obs","Number of observations to View:",10)
),
mainPanel(
verbatimTextOutput("summary"),
tableOutput("view")
)
)
)
server1<-shinyServer(function(input,output){
datasetInput<-reactive({
switch (input$dataset,
"rock" = rock,
"pressure" = pressure,
"cars" = cars)
})
output$summary<-renderPrint({
dataset<-datasetInput()
summary(dataset)
}),
library(datasets)
ui1<-shinyUI(
pageWithSidebar(
headerPanel("Hello Shiny"),
sidebarPanel(
selectInput("dataset","choose a dataset:", choices = c("rock","pressure","cars")),
numericInput("obs","Number of observations to View:",10)
),
mainPanel(
verbatimTextOutput("summary"),
tableOutput("view")
)
)
)
server1<-shinyServer(function(input,output){
datasetInput<-reactive({
switch (input$dataset,
"rock" = rock,
"pressure" = pressure,
"cars" = cars)
})
output$summary<-renderPrint({
dataset<-datasetInput()
summary(dataset)
})
output$view<-renderTable({
head(datasetInput(),n=input$obs)
})
}
)
shinyApp(ui1,server1)
runExample("03_reactivity")
ui2<-shinyUI(
pageWithSidebar(
headerPanel("Reactivity"),
sidebarPanel(
textInput("caption","Caption","Data Summary"),
selectInput("dataset","choose a dataset:", choices = C("rock","pressure","cars")),
numericInput("obs","Number of Oservations to View:",10)
),
mainPanel(
h3(textOutput("caption")),
verbatimTextOutput("summary"),
tableOutput("view")
)
)
)
ui2<-shinyUI(
pageWithSidebar(
headerPanel("Reactivity"),
sidebarPanel(
textInput("caption","Caption","Data Summary"),
selectInput("dataset","choose a dataset:", choices = c("rock","pressure","cars")),
numericInput("obs","Number of Oservations to View:",10)
),
mainPanel(
h3(textOutput("caption")),
verbatimTextOutput("summary"),
tableOutput("view")
)
)
)
server2<-shinyServer(
function(input,output){
datasetInput<-reactive({
switch(input$dataset,
"rock"=rock,
"pressure"= pressure,
"cars"=cars)
})
output$caption<-renderText({
input$caption
})
output$summary<-renderPrint({
dataset<-datasetInput()
summary(dataset)
})
output$view<-renderTable({
head(datasetInput(),n=input$obs)
})
}
)
shinyApp(ui2,server2)
